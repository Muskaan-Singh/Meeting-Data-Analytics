{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muskaan-Singh/Meeting-Data-Analytics/blob/main/Minuting_Datasets_Raw_Data_Info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERS67ch6JFtj",
        "outputId": "d71cfd1c-0baa-42ed-af70-9bd3c3b58e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "from nltk.tag import pos_tag # for proper noun\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from statistics import median\n",
        "import math\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04poYaW2JJRc"
      },
      "outputs": [],
      "source": [
        "path='/content/drive/MyDrive/Kaggle/Cpastone_text_minuting/ami/transcripts'\n",
        "filelist=os.listdir(path)\n",
        "transcripts={}\n",
        "for file in filelist:\n",
        "    with open(f'{path}/{file}', 'r') as f: \n",
        "        txt=f.read()\n",
        "        transcripts[file.split('.')[0]]=txt\n",
        "\n",
        "path='/content/drive/MyDrive/Kaggle/Cpastone_text_minuting/ami/summaries/extractive'\n",
        "filelist=os.listdir(path)       \n",
        "extractive_summary={}\n",
        "for file in filelist:\n",
        "    with open(f'{path}/{file}', 'r') as f: \n",
        "        txt=f.read()\n",
        "        extractive_summary[file.split('.')[0]]=txt \n",
        "        \n",
        "path='/content/drive/MyDrive/Kaggle/Cpastone_text_minuting/ami/summaries/abstractive'\n",
        "filelist=os.listdir(path)       \n",
        "abstractive_summary={}\n",
        "for file in filelist:\n",
        "    with open(f'{path}/{file}', 'r') as f: \n",
        "        txt=f.read()\n",
        "        abstractive_summary[file.split('.')[0]]=txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peelcHQnJ-j5"
      },
      "outputs": [],
      "source": [
        "inner_join_transcripts=deepcopy(transcripts)\n",
        "inner_join_extractive_summary=deepcopy(extractive_summary)\n",
        "inner_join_abstractive_summary=deepcopy(abstractive_summary)\n",
        "l=list(transcripts.keys())\n",
        "\n",
        "for i in l:\n",
        "    if i not in inner_join_abstractive_summary:\n",
        "        del inner_join_transcripts[i]\n",
        "        if i in inner_join_extractive_summary:\n",
        "            del inner_join_extractive_summary[i]\n",
        "        continue\n",
        "    if i not in inner_join_extractive_summary:\n",
        "        del inner_join_transcripts[i]\n",
        "        if i in inner_join_abstractive_summary:\n",
        "            del inner_join_abstractive_summary[i]\n",
        "\n",
        "def ami_mean(transcripts, small_threshold , big_threshold ):\n",
        "    stopWords = list(set(stopwords.words(\"english\")))\n",
        "    count_small=0\n",
        "    count_big=0\n",
        "    sum_small=0\n",
        "    sum_big=0\n",
        "    position_small=[]\n",
        "    position_big=[]\n",
        "    total_sent=0\n",
        "    total_words=0\n",
        "    total_refined_words=0\n",
        "    unique={}\n",
        "    number=len(transcripts)\n",
        "    total_unique_refined_words=0\n",
        "    max_len=[]\n",
        "    position_max_len=[]\n",
        "    small_bool=[]\n",
        "    turns=0\n",
        "    for key in transcripts.keys():\n",
        "        txt=nltk.sent_tokenize(transcripts[key])\n",
        "        total_sent+=len(txt)\n",
        "        m=0\n",
        "        p=0\n",
        "        for index,sentence in enumerate(txt):\n",
        "            temp=nltk.word_tokenize(sentence)\n",
        "            sen_len=len(temp)\n",
        "            m=max(m,sen_len)\n",
        "            if m==sen_len:\n",
        "                p=index/len(txt)\n",
        "            temp=[word.lower() for word in temp]\n",
        "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "            for word in word_tokens_refined:\n",
        "                if word not in unique: \n",
        "                    total_unique_refined_words+=1\n",
        "                    unique[word]=1\n",
        "                elif word in unique:\n",
        "                    unique[word]+=1\n",
        "\n",
        "            total_refined_words+=len(word_tokens_refined)\n",
        "            total_words+=sen_len\n",
        "            \n",
        "            if sen_len<small_threshold:\n",
        "                count_small+=1\n",
        "                sum_small+=sen_len\n",
        "                position_small.append(index/len(txt))\n",
        "                small_bool.append(1)\n",
        "            elif sen_len>big_threshold:\n",
        "                count_big+=1\n",
        "                sum_big+=sen_len\n",
        "                position_big.append(index/len(txt))\n",
        "                small_bool.append(0)\n",
        "            else:\n",
        "                small_bool.append(0)\n",
        "                \n",
        "        max_len.append(m)\n",
        "        position_max_len.append(p)\n",
        "\n",
        "    # number of continuous occurances\n",
        "    small_cont=0\n",
        "    check=False\n",
        "    for i in range(0,len(small_bool),4):\n",
        "        if sum(small_bool[i:i+4])>=2:\n",
        "            small_cont+=1\n",
        "            check=False\n",
        "        else:\n",
        "            try: \n",
        "                if sum(small_bool[i:i+4])==1 and small_bool[i+3]==1:\n",
        "                    check=True\n",
        "                elif sum(small_bool[i:i+4])==1 and small_bool[i]==1 and check==True:\n",
        "                    small_cont+=1\n",
        "                    check=False            \n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    \n",
        "    \n",
        "    return (count_small, count_big, sum_small, sum_big, position_small, position_big, \n",
        "total_sent, total_words, total_refined_words, total_unique_refined_words, unique, number, max_len,\n",
        "            position_max_len, small_cont)\n",
        "\n",
        "(count_small_transcripts_ami, count_big_transcripts_ami, sum_small_transcripts_ami, \n",
        "sum_big_transcripts_ami, position_small_transcripts_ami, position_big_transcripts_ami, \n",
        "total_sent_transcripts_ami, total_words_transcripts_ami, total_refined_words_transcripts_ami,\n",
        "total_refined_unique_words_transcripts_ami, unique_dict_transcripts_ami, number_transcripts_ami,\n",
        "max_len_transcripts_ami, position_max_len_transcripts_ami, small_cont_transcripts_ami) = ami_mean(transcripts, 5 , 10)\n",
        "\n",
        "(count_small_esum_ami, count_big_esum_ami, sum_small_esum_ami, \n",
        "sum_big_esum_ami, position_small_esum_ami, position_big_esum_ami, \n",
        "total_sent_esum_ami, total_words_esum_ami, total_refined_words_esum_ami,\n",
        "total_refined_unique_words_esum_ami, unique_dict_esum_ami,number_esum_ami,\n",
        "max_len_esum_ami, position_max_len_esum_ami,small_cont_esum_ami)  = ami_mean(extractive_summary, 5 , 10)\n",
        "\n",
        "(count_small_asum_ami, count_big_asum_ami, sum_small_asum_ami, \n",
        "sum_big_asum_ami, position_small_asum_ami, position_big_asum_ami, \n",
        "total_sent_asum_ami, total_words_asum_ami, total_refined_words_asum_ami,\n",
        "total_refined_unique_words_asum_ami, unique_dict_asum_ami,number_asum_ami,\n",
        "max_len_asum_ami, position_max_len_asum_ami,small_cont_asum_ami) = ami_mean(abstractive_summary, 5 , 10)\n",
        "\n",
        "# number of hours:\n",
        "ami_hours=100  #from their website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrKHYp2UKEsp",
        "outputId": "33d13c5e-0e02-4e8c-d2b3-2658b7a6f684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ICSI'...\n",
            "remote: Enumerating objects: 4548, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 4548 (delta 2), reused 12 (delta 2), pack-reused 4536\u001b[K\n",
            "Receiving objects: 100% (4548/4548), 56.97 MiB | 14.90 MiB/s, done.\n",
            "Resolving deltas: 100% (2010/2010), done.\n",
            "Checking out files: 100% (4576/4576), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/saprativa/ICSI.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm849otbKKFC"
      },
      "outputs": [],
      "source": [
        "\"\"\"# ICSI Dataset\"\"\"\n",
        "\n",
        "path='/content/ICSI/ICSI_original_transcripts/transcripts'\n",
        "temp=os.listdir(path)\n",
        "root_list=[]\n",
        "meeting_name_list=[]\n",
        "text_list=[]\n",
        "time_list=[]\n",
        "participants_count_list=[]\n",
        "turns=[]\n",
        "\n",
        "path1='/content/ICSI/ICSIplus/Contributions/Summarization/abstractive'\n",
        "absumfilelist=os.listdir(path1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRciFAJvKQRb",
        "outputId": "7f809122-d18b-4f98-809b-2a54951a96f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
          ]
        }
      ],
      "source": [
        "filelist=[]\n",
        "for file in absumfilelist:\n",
        "    if f'{file[:-12]}.mrt' in temp:\n",
        "        filelist.append(f'{file[:-12]}.mrt')\n",
        "\n",
        "\n",
        "for file in filelist:\n",
        "    if file[-3:]=='mrt':\n",
        "        tree = ET.parse(f'{path}/{file}')\n",
        "        root = tree.getroot()\n",
        "        root_list.append(root)\n",
        "        meeting_name=root.attrib['Session']\n",
        "        meeting_name_list.append(meeting_name)\n",
        "        time = float(root[1].attrib['EndTime'])-float(root[1].attrib['StartTime'])\n",
        "        time_list.append(time)\n",
        "        for index,child in enumerate(root[0]):\n",
        "            if child.tag=='Participants':\n",
        "                participants_count_list.append(len(root[0][index].getchildren()))\n",
        "        txt=[]\n",
        "        t=0\n",
        "        # get current speaker\n",
        "        for child in root[1]:\n",
        "            try:\n",
        "                current_speaker=child.attrib['Participant']\n",
        "                break\n",
        "            except:\n",
        "                continue\n",
        "        for child in root[1]:\n",
        "            try:\n",
        "                new_speaker=child.attrib['Participant']\n",
        "            except:\n",
        "                pass\n",
        "            if current_speaker!=new_speaker:\n",
        "                t+=1\n",
        "                current_speaker=new_speaker\n",
        "            txt.append(child.text)\n",
        "        text_list.append(txt)\n",
        "        turns.append(t)\n",
        "        \n",
        "ab_sumroot_list=[]\n",
        "ab_sumtext_list=[]\n",
        "for f in absumfilelist:\n",
        "    tree = ET.parse(f'{path1}/{f}')\n",
        "    root = tree.getroot()\n",
        "    ab_sumroot_list.append(root)\n",
        "    txt=[]\n",
        "    s=[]\n",
        "    for child in ab_sumroot_list[0]:\n",
        "        for i in child:\n",
        "            txt.append(i.text)\n",
        "    ab_sumtext_list.append(txt)\n",
        "\n",
        "icsi_hours=sum(time_list)/3600\n",
        "\n",
        "def icsi_mean(text_list,small_threshold, big_threshold):\n",
        "    stopWords = list(set(stopwords.words(\"english\")))\n",
        "    count_small=0\n",
        "    count_big=0\n",
        "    sum_small=0\n",
        "    sum_big=0\n",
        "    position_small=[]\n",
        "    position_big=[]\n",
        "    total_sent=0\n",
        "    total_words=0\n",
        "    total_refined_words=0\n",
        "    unique={}\n",
        "    total_unique_refined_words=0\n",
        "    number=len(text_list)\n",
        "    \n",
        "    max_len=[]\n",
        "    position_max_len=[]\n",
        "    \n",
        "    small_bool=[]\n",
        "    for i in range(len(text_list)):\n",
        "        # index of one document \n",
        "        \n",
        "        total_sent+=len(text_list[i])\n",
        "        m=0\n",
        "        p=0\n",
        "        #m=max(m,len(text_list[i]))\n",
        "        #if m==len(text_list[i]):\n",
        "        #    p=i/\n",
        "        for index,text in enumerate(text_list[i]):\n",
        "            \n",
        "            a=nltk.word_tokenize(text)\n",
        "            m=max(m,len(a))\n",
        "            if m==len(a):\n",
        "                p=index/len(text_list[i])\n",
        "            a=[word.lower() for word in a]\n",
        "            word_tokens_refined=[x for x in a if x not in stopWords]\n",
        "            for word in word_tokens_refined:\n",
        "                if word not in unique: \n",
        "                    total_unique_refined_words+=1\n",
        "                    unique[word]=1\n",
        "                elif word in unique:\n",
        "                    unique[word]+=1\n",
        "            total_refined_words+=len(word_tokens_refined)\n",
        "            total_words+=len(a)\n",
        "            \n",
        "            if len(a)<small_threshold:\n",
        "                count_small+=1\n",
        "                sum_small+=len(a)\n",
        "                position_small.append(index/len(text_list[i]))\n",
        "                small_bool.append(1)\n",
        "            elif len(a)>big_threshold:\n",
        "                count_big+=1\n",
        "                sum_big+=len(a)\n",
        "                position_big.append(index/len(text_list[i]))\n",
        "                small_bool.append(0)\n",
        "            else:\n",
        "                small_bool.append(0)\n",
        "            max_len.append(m)\n",
        "            position_max_len.append(p)\n",
        "     \n",
        "    small_cont=0\n",
        "    check=False\n",
        "    for i in range(0,len(small_bool),4):\n",
        "        if sum(small_bool[i:i+4])>=2:\n",
        "            small_cont+=1\n",
        "            check=False\n",
        "        else:\n",
        "            try:\n",
        "                if sum(small_bool[i:i+4])==1 and small_bool[i+3]==1:\n",
        "                    check=True\n",
        "                elif sum(small_bool[i:i+4])==1 and small_bool[i]==1 and check==True:\n",
        "                    small_cont+=1\n",
        "                    check=False  \n",
        "            except:\n",
        "                pass\n",
        "\n",
        "                \n",
        "    return (count_small, count_big, sum_small, sum_big, position_small, position_big, total_sent,total_words,\n",
        "total_refined_words, total_unique_refined_words, unique, number, max_len, position_max_len, small_cont)\n",
        "\n",
        "(count_small_transcripts_icsi, count_big_transcripts_icsi, sum_small_transcripts_icsi,\n",
        "sum_big_transcripts_icsi, position_small_transcripts_icsi, position_big_transcripts_icsi,\n",
        "total_sent_transcripts_icsi,total_words_transcripts_icsi, total_refined_words_transcripts_icsi, \n",
        "total_refined_unique_words_transcripts_icsi, unique_dict_transcripts_icsi, number_transcripts_icsi, \n",
        "max_len_transcripts_icsi, position_max_len_transcripts_icsi, small_cont_transcripts_icsi)=icsi_mean(text_list,5,10)\n",
        "\n",
        "(count_small_asum_icsi, count_big_asum_icsi, sum_small_asum_icsi,\n",
        "sum_big_asum_icsi, position_small_asum_icsi, position_big_asum_icsi,\n",
        "total_sent_asum_icsi,total_words_asum_icsi, total_refined_words_asum_icsi, \n",
        "total_refined_unique_words_asum_icsi, unique_dict_asum_icsi, number_asum_icsi,\n",
        "max_len_asum_icsi, position_max_len_asum_icsi, small_count_asum_icsi)=icsi_mean(ab_sumtext_list,5,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TRWy6uKKUlD",
        "outputId": "c9f80d31-4679-4622-b6a6-315be2e59efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Kaggle/Cpastone_text_minuting/AutoMin_combined_all.zip\n",
            "   creating: train/\n",
            "   creating: train/meeting_en_test_013/\n",
            "  inflating: train/meeting_en_test_013/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_013/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_006/\n",
            "  inflating: train/meeting_cs_dev_006/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_006/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_012_1/\n",
            "  inflating: train/meeting_en_train_012_1/transcript_MAN_annot21.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_012_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_027_1/\n",
            "  inflating: train/meeting_cs_train_027_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_027_1/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_test_009/\n",
            "  inflating: train/meeting_en_test_009/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_009/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_063/\n",
            "  inflating: train/meeting_en_train_063/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_063/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_015_1/\n",
            "  inflating: train/meeting_en_train_015_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_015_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_009_1/\n",
            "  inflating: train/meeting_cs_train_009_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_009_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_005/\n",
            "  inflating: train/meeting_en_dev_005/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_005/transcript_MAN_annot16.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_001_1/\n",
            "  inflating: train/meeting_cs_dev_001_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_001_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_009_2/\n",
            "  inflating: train/meeting_cs_dev_009_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_009_2/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_024_1/\n",
            "  inflating: train/meeting_cs_train_024_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_024_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_048/\n",
            "  inflating: train/meeting_en_train_048/transcript_MAN_annot01.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_048/minutes_GENER_annot01.deidentified.txt  \n",
            "   creating: train/meeting_en_train_053_1/\n",
            "  inflating: train/meeting_en_train_053_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_053_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_013/\n",
            "  inflating: train/meeting_en_train_013/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_013/transcript_MAN2_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002/\n",
            "  inflating: train/meeting_en_test_002/minutes_GENER_annot21.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002/transcript_MAN2_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_016_1/\n",
            "  inflating: train/meeting_en_train_016_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_016_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_007_1/\n",
            "  inflating: train/meeting_cs_dev_007_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_007_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_010_1/\n",
            "  inflating: train/meeting_cs_train_010_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_010_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_018/\n",
            "  inflating: train/meeting_en_train_018/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_018/transcript_MAN2_annot23.deidentified.txt  \n",
            "   creating: train/meeting_en_test_007_1/\n",
            "  inflating: train/meeting_en_test_007_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_007_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_009_1/\n",
            "  inflating: train/meeting_en_train_009_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_009_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_028_1/\n",
            "  inflating: train/meeting_cs_train_028_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_028_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_007/\n",
            "  inflating: train/meeting_en_test_007/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_007/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_006_2/\n",
            "  inflating: train/meeting_cs_test_006_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_006_2/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_8/\n",
            "  inflating: train/meeting_en_test_002_8/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_8/minutes_GENER_annot23.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_010/\n",
            "  inflating: train/meeting_cs_dev_010/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_010/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_031_1/\n",
            "  inflating: train/meeting_cs_train_031_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_031_1/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_032_1/\n",
            "  inflating: train/meeting_cs_train_032_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_032_1/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_018_1/\n",
            "  inflating: train/meeting_cs_train_018_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_018_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_042_1/\n",
            "  inflating: train/meeting_en_train_042_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_042_1/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_003_2/\n",
            "  inflating: train/meeting_cs_dev_003_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_003_2/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_007/\n",
            "  inflating: train/meeting_cs_train_007/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_007/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_016/\n",
            "  inflating: train/meeting_en_train_016/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_016/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_002/\n",
            "  inflating: train/meeting_cs_dev_002/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_002/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_015_1/\n",
            "  inflating: train/meeting_en_test_015_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_015_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_023/\n",
            "  inflating: train/meeting_en_train_023/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_023/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_067/\n",
            "  inflating: train/meeting_en_train_067/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_067/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_006_2/\n",
            "  inflating: train/meeting_cs_dev_006_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_006_2/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_025_1/\n",
            "  inflating: train/meeting_en_train_025_1/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_025_1/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_train_011/\n",
            "  inflating: train/meeting_en_train_011/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_011/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_005_2/\n",
            "  inflating: train/meeting_cs_test_005_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_005_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_013/\n",
            "  inflating: train/meeting_cs_train_013/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_013/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_008_1/\n",
            "  inflating: train/meeting_cs_train_008_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_008_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_017_1/\n",
            "  inflating: train/meeting_en_train_017_1/transcript_MAN_annot03.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_017_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_10/\n",
            "  inflating: train/meeting_en_test_002_10/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_10/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_074_1/\n",
            "  inflating: train/meeting_en_train_074_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_074_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_008_1/\n",
            "  inflating: train/meeting_cs_test_008_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_008_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_070/\n",
            "  inflating: train/meeting_en_train_070/minutes_GENER_annot21.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_070/transcript_MAN_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_014_1/\n",
            "  inflating: train/meeting_cs_train_014_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_014_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_6/\n",
            "  inflating: train/meeting_en_test_002_6/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_6/minutes_GENER_annot99.deidentified.txt  \n",
            "   creating: train/meeting_en_train_003_1/\n",
            "  inflating: train/meeting_en_train_003_1/transcript_MAN2_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_003_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_003/\n",
            "  inflating: train/meeting_cs_test_003/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_003/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_035/\n",
            "  inflating: train/meeting_en_train_035/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_035/transcript_MAN2_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_014/\n",
            "  inflating: train/meeting_en_train_014/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_014/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_063_1/\n",
            "  inflating: train/meeting_en_train_063_1/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_063_1/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_010_1/\n",
            "  inflating: train/meeting_cs_dev_010_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_010_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_021_1/\n",
            "  inflating: train/meeting_cs_train_021_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_021_1/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_005_2/\n",
            "  inflating: train/meeting_cs_train_005_2/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_005_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_055_2/\n",
            "  inflating: train/meeting_en_train_055_2/transcript_MAN2_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_055_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_015/\n",
            "  inflating: train/meeting_cs_train_015/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_015/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_003_3/\n",
            "  inflating: train/meeting_cs_dev_003_3/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_003_3/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_test_014_1/\n",
            "  inflating: train/meeting_en_test_014_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_014_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_019_1/\n",
            "  inflating: train/meeting_cs_train_019_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_019_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_014_2/\n",
            "  inflating: train/meeting_cs_train_014_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_014_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_062/\n",
            "  inflating: train/meeting_en_train_062/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_062/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_010_2/\n",
            "  inflating: train/meeting_cs_dev_010_2/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_010_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_045/\n",
            "  inflating: train/meeting_en_train_045/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_045/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_033_1/\n",
            "  inflating: train/meeting_en_train_033_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_033_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_076/\n",
            "  inflating: train/meeting_en_train_076/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_076/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_011_1/\n",
            "  inflating: train/meeting_cs_test_011_1/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_011_1/minutes_GENER_annot05.txt  \n",
            "   creating: train/meeting_en_train_050_1/\n",
            "  inflating: train/meeting_en_train_050_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_050_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_058_1/\n",
            "  inflating: train/meeting_en_train_058_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_058_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_077_1/\n",
            "  inflating: train/meeting_en_train_077_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_077_1/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_026/\n",
            "  inflating: train/meeting_en_train_026/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_026/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_034/\n",
            "  inflating: train/meeting_en_train_034/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_034/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_071/\n",
            "  inflating: train/meeting_en_train_071/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_071/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_019_1/\n",
            "  inflating: train/meeting_en_train_019_1/minutes_GENER_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_019_1/transcript_MAN_annot14.deidentified.txt  \n",
            "   creating: train/meeting_en_train_084_2/\n",
            "  inflating: train/meeting_en_train_084_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_084_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_008/\n",
            "  inflating: train/meeting_en_dev_008/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_008/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_train_080_1/\n",
            "  inflating: train/meeting_en_train_080_1/minutes_GENER_annot12.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_080_1/transcript_MAN2_annot12.deidentified.txt  \n",
            "   creating: train/meeting_en_train_021/\n",
            "  inflating: train/meeting_en_train_021/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_021/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_008_1/\n",
            "  inflating: train/meeting_cs_dev_008_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_008_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_012_1/\n",
            "  inflating: train/meeting_cs_train_012_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_012_1/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_009_2/\n",
            "  inflating: train/meeting_cs_test_009_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_009_2/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_008/\n",
            "  inflating: train/meeting_en_train_008/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_008/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_004_1/\n",
            "  inflating: train/meeting_en_dev_004_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_004_1/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_077/\n",
            "  inflating: train/meeting_en_train_077/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_077/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_031/\n",
            "  inflating: train/meeting_en_train_031/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_031/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_train_064/\n",
            "  inflating: train/meeting_en_train_064/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_064/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_003/\n",
            "  inflating: train/meeting_en_test_003/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_003/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_008/\n",
            "  inflating: train/meeting_cs_dev_008/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_008/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_010_3/\n",
            "  inflating: train/meeting_cs_dev_010_3/transcript_MAN2_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_010_3/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_058/\n",
            "  inflating: train/meeting_en_train_058/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_058/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_012/\n",
            "  inflating: train/meeting_en_test_012/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_012/transcript_MAN2_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_030_1/\n",
            "  inflating: train/meeting_cs_train_030_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_030_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_004/\n",
            "  inflating: train/meeting_cs_test_004/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_004/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_016_3/\n",
            "  inflating: train/meeting_en_test_016_3/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_016_3/minutes_GENER_annot01.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_006_2/\n",
            "  inflating: train/meeting_en_dev_006_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_006_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_3/\n",
            "  inflating: train/meeting_en_dev_010_3/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_3/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_001_1/\n",
            "  inflating: train/meeting_cs_test_001_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_001_1/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_051_1/\n",
            "  inflating: train/meeting_en_train_051_1/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_051_1/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_train_002/\n",
            "  inflating: train/meeting_en_train_002/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_002/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_080/\n",
            "  inflating: train/meeting_en_train_080/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_080/transcript_MAN2_annot12.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_7/\n",
            "  inflating: train/meeting_en_train_085_7/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_7/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_055_1/\n",
            "  inflating: train/meeting_en_train_055_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_055_1/transcript_MAN2_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_1/\n",
            "  inflating: train/meeting_en_train_085_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_1/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_024/\n",
            "  inflating: train/meeting_en_train_024/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_024/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_069/\n",
            "  inflating: train/meeting_en_train_069/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_069/transcript_MAN2_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_9/\n",
            "  inflating: train/meeting_en_test_002_9/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_9/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_5/\n",
            "  inflating: train/meeting_en_dev_010_5/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_5/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_en_train_051/\n",
            "  inflating: train/meeting_en_train_051/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_051/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_train_004/\n",
            "  inflating: train/meeting_en_train_004/transcript_MAN_annot22.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_004/minutes_GENER_annot22.deidentified.txt  \n",
            "   creating: train/meeting_en_train_084/\n",
            "  inflating: train/meeting_en_train_084/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_084/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_072_2/\n",
            "  inflating: train/meeting_en_train_072_2/transcript_MAN2_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_072_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_2/\n",
            "  inflating: train/meeting_en_dev_010_2/minutes_GENER_annot97.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_2/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_002_3/\n",
            "  inflating: train/meeting_cs_test_002_3/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_002_3/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_007/\n",
            "  inflating: train/meeting_en_train_007/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_007/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_009_1/\n",
            "  inflating: train/meeting_en_dev_009_1/transcript_MAN2_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_009_1/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_060_1/\n",
            "  inflating: train/meeting_en_train_060_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_060_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_049/\n",
            "  inflating: train/meeting_en_train_049/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_049/transcript_MAN2_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_2/\n",
            "  inflating: train/meeting_en_test_002_2/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_2/transcript_MAN2_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_076_1/\n",
            "  inflating: train/meeting_en_train_076_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_076_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_030_1/\n",
            "  inflating: train/meeting_en_train_030_1/transcript_MAN_annot01.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_030_1/minutes_GENER_annot01.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_030/\n",
            "  inflating: train/meeting_cs_train_030/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_030/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_016_2/\n",
            "  inflating: train/meeting_en_test_016_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_016_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_025_1/\n",
            "  inflating: train/meeting_cs_train_025_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_025_1/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_005/\n",
            "  inflating: train/meeting_en_test_005/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_005/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_043/\n",
            "  inflating: train/meeting_en_train_043/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_043/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_028/\n",
            "  inflating: train/meeting_cs_train_028/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_028/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_009/\n",
            "  inflating: train/meeting_cs_train_009/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_009/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_008/\n",
            "  inflating: train/meeting_en_test_008/transcript_MAN2_annot22.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_008/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_041/\n",
            "  inflating: train/meeting_en_train_041/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_041/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_7/\n",
            "  inflating: train/meeting_en_test_002_7/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_7/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_001_1/\n",
            "  inflating: train/meeting_en_train_001_1/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_001_1/transcript_MAN2_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_test_017_1/\n",
            "  inflating: train/meeting_en_test_017_1/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_017_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_003_1/\n",
            "  inflating: train/meeting_cs_test_003_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_003_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_006_1/\n",
            "  inflating: train/meeting_en_test_006_1/minutes_GENER_annot16.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_006_1/transcript_MAN_annot16.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_004/\n",
            "  inflating: train/meeting_cs_train_004/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_004/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_test_006/\n",
            "  inflating: train/meeting_en_test_006/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_006/transcript_MAN_annot16.deidentified.txt  \n",
            "   creating: train/meeting_en_train_060/\n",
            "  inflating: train/meeting_en_train_060/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_060/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_007_2/\n",
            "  inflating: train/meeting_cs_test_007_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_007_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_019_2/\n",
            "  inflating: train/meeting_cs_train_019_2/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_019_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_081/\n",
            "  inflating: train/meeting_en_train_081/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_081/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_test_011_1/\n",
            "  inflating: train/meeting_en_test_011_1/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_011_1/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_test_018_2/\n",
            "  inflating: train/meeting_en_test_018_2/transcript_MAN_annot23.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_018_2/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_081_1/\n",
            "  inflating: train/meeting_en_train_081_1/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_081_1/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_train_042/\n",
            "  inflating: train/meeting_en_train_042/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_042/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_006_1/\n",
            "  inflating: train/meeting_cs_train_006_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_006_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_017_2/\n",
            "  inflating: train/meeting_cs_train_017_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_017_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_030_2/\n",
            "  inflating: train/meeting_cs_train_030_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_030_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_057/\n",
            "  inflating: train/meeting_en_train_057/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_057/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_083/\n",
            "  inflating: train/meeting_en_train_083/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_083/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_016/\n",
            "  inflating: train/meeting_cs_train_016/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_016/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_010_1/\n",
            "  inflating: train/meeting_cs_test_010_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_010_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_040/\n",
            "  inflating: train/meeting_en_train_040/minutes_GENER_annot10.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_040/transcript_MAN_annot10.deidentified.txt  \n",
            "   creating: train/meeting_en_train_073/\n",
            "  inflating: train/meeting_en_train_073/transcript_MAN_annot22.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_073/minutes_GENER_annot22.deidentified.txt  \n",
            "   creating: train/meeting_en_test_010_2/\n",
            "  inflating: train/meeting_en_test_010_2/transcript_MAN_annot03.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_010_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_008_2/\n",
            " extracting: train/meeting_cs_train_008_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_008_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_056_1/\n",
            "  inflating: train/meeting_en_train_056_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_056_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_035_2/\n",
            "  inflating: train/meeting_en_train_035_2/transcript_MAN2_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_035_2/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_071_1/\n",
            "  inflating: train/meeting_en_train_071_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_071_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_015/\n",
            "  inflating: train/meeting_en_test_015/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_015/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_5/\n",
            "  inflating: train/meeting_en_train_085_5/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_5/minutes_GENER_annot99.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_001/\n",
            "  inflating: train/meeting_cs_test_001/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_001/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_test_003_2/\n",
            "  inflating: train/meeting_en_test_003_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_003_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_036/\n",
            "  inflating: train/meeting_en_train_036/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_036/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_036_1/\n",
            "  inflating: train/meeting_en_train_036_1/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_036_1/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_065/\n",
            "  inflating: train/meeting_en_train_065/minutes_GENER_annot12.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_065/transcript_MAN_annot12.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_029/\n",
            "  inflating: train/meeting_cs_train_029/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_029/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_022_1/\n",
            "  inflating: train/meeting_en_train_022_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_022_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_004/\n",
            "  inflating: train/meeting_en_dev_004/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_004/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_028_1/\n",
            "  inflating: train/meeting_en_train_028_1/transcript_MAN_annot15.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_028_1/minutes_GENER_annot15.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_7/\n",
            "  inflating: train/meeting_en_dev_010_7/minutes_GENER_annot01.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_7/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_en_train_056/\n",
            "  inflating: train/meeting_en_train_056/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_056/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_test_016/\n",
            "  inflating: train/meeting_en_test_016/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_016/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_049_1/\n",
            "  inflating: train/meeting_en_train_049_1/transcript_MAN2_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_049_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_075_1/\n",
            "  inflating: train/meeting_en_train_075_1/transcript_MAN3_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_075_1/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_024/\n",
            "  inflating: train/meeting_cs_train_024/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_024/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_001_1/\n",
            "  inflating: train/meeting_en_test_001_1/transcript_MAN2_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_001_1/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_010_1/\n",
            "  inflating: train/meeting_en_train_010_1/minutes_GENER_annot12.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_010_1/transcript_MAN_annot12.deidentified.txt  \n",
            "   creating: train/meeting_en_train_021_1/\n",
            "  inflating: train/meeting_en_train_021_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_021_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_6/\n",
            "  inflating: train/meeting_en_dev_010_6/minutes_GENER_annot95.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_6/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_011_1/\n",
            "  inflating: train/meeting_cs_train_011_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_011_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_007_2/\n",
            "  inflating: train/meeting_cs_dev_007_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_007_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_007/\n",
            "  inflating: train/meeting_en_dev_007/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_007/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_007_1/\n",
            "  inflating: train/meeting_cs_train_007_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_007_1/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_028_2/\n",
            "  inflating: train/meeting_cs_train_028_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_028_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_023_1/\n",
            "  inflating: train/meeting_en_train_023_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_023_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_016_1/\n",
            "  inflating: train/meeting_cs_test_016_1/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_016_1/minutes_GENER_annot05.txt  \n",
            "   creating: train/meeting_en_train_082_1/\n",
            "  inflating: train/meeting_en_train_082_1/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_082_1/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_007_1/\n",
            "  inflating: train/meeting_cs_test_007_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_007_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_4/\n",
            "  inflating: train/meeting_en_train_085_4/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_4/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_008_2/\n",
            "  inflating: train/meeting_cs_dev_008_2/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_008_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_046/\n",
            "  inflating: train/meeting_en_train_046/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_046/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_008/\n",
            "  inflating: train/meeting_cs_train_008/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_008/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_016_1/\n",
            "  inflating: train/meeting_cs_train_016_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_016_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_008/\n",
            "  inflating: train/meeting_cs_test_008/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_008/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_002/\n",
            "  inflating: train/meeting_cs_train_002/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_002/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_007_1/\n",
            "  inflating: train/meeting_en_train_007_1/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_007_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085/\n",
            "  inflating: train/meeting_en_train_085/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085/minutes_GENER_annot11.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_002_2/\n",
            "  inflating: train/meeting_cs_test_002_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_002_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_005_2/\n",
            "  inflating: train/meeting_cs_dev_005_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_005_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_016_2/\n",
            "  inflating: train/meeting_cs_train_016_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_016_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_009_3/\n",
            "  inflating: train/meeting_en_dev_009_3/transcript_MAN2_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_009_3/minutes_GENER_annot01.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_004/\n",
            "  inflating: train/meeting_cs_dev_004/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_004/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_039/\n",
            "  inflating: train/meeting_en_train_039/transcript_MAN2_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_039/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_084_1/\n",
            "  inflating: train/meeting_en_train_084_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_084_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_006/\n",
            "  inflating: train/meeting_cs_train_006/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_006/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_026/\n",
            "  inflating: train/meeting_cs_train_026/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_026/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_061/\n",
            "  inflating: train/meeting_en_train_061/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_061/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_047/\n",
            "  inflating: train/meeting_en_train_047/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_047/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_017/\n",
            "  inflating: train/meeting_cs_train_017/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_017/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_003_4/\n",
            "  inflating: train/meeting_cs_dev_003_4/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_003_4/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_045_1/\n",
            "  inflating: train/meeting_en_train_045_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_045_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_002_1/\n",
            "  inflating: train/meeting_en_train_002_1/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_002_1/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_en_train_035_1/\n",
            "  inflating: train/meeting_en_train_035_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_035_1/transcript_MAN2_annot17.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_006_4/\n",
            "  inflating: train/meeting_cs_test_006_4/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_006_4/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_032/\n",
            "  inflating: train/meeting_cs_train_032/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_032/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_067_1/\n",
            "  inflating: train/meeting_en_train_067_1/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_067_1/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_014/\n",
            "  inflating: train/meeting_cs_train_014/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_014/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_004_2/\n",
            "  inflating: train/meeting_cs_dev_004_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_004_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_009/\n",
            "  inflating: train/meeting_en_dev_009/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_009/transcript_MAN2_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_006_3/\n",
            "  inflating: train/meeting_cs_test_006_3/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_006_3/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_018/\n",
            "  inflating: train/meeting_en_test_018/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_018/transcript_MAN_annot23.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_004_1/\n",
            "  inflating: train/meeting_cs_dev_004_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_004_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010/\n",
            "  inflating: train/meeting_en_dev_010/minutes_GENER_annot21.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_005/\n",
            "  inflating: train/meeting_cs_test_005/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_005/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_006_1/\n",
            "  inflating: train/meeting_cs_dev_006_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_006_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_003_1/\n",
            "  inflating: train/meeting_en_test_003_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_003_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_009_2/\n",
            "  inflating: train/meeting_cs_train_009_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_009_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_037/\n",
            "  inflating: train/meeting_en_train_037/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_037/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_053/\n",
            "  inflating: train/meeting_en_train_053/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_053/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_052/\n",
            "  inflating: train/meeting_en_train_052/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_052/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_010/\n",
            "  inflating: train/meeting_cs_train_010/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_010/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_033/\n",
            "  inflating: train/meeting_cs_train_033/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_033/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_005/\n",
            "  inflating: train/meeting_en_train_005/transcript_MAN_annot15.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_005/minutes_GENER_annot15.deidentified.txt  \n",
            "   creating: train/meeting_en_test_010_1/\n",
            "  inflating: train/meeting_en_test_010_1/transcript_MAN_annot03.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_010_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_041_1/\n",
            "  inflating: train/meeting_en_train_041_1/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_041_1/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_052_1/\n",
            "  inflating: train/meeting_en_train_052_1/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_052_1/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_012/\n",
            "  inflating: train/meeting_cs_train_012/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_012/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_027/\n",
            "  inflating: train/meeting_en_train_027/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_027/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_018/\n",
            "  inflating: train/meeting_cs_train_018/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_018/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_005_1/\n",
            "  inflating: train/meeting_en_test_005_1/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_005_1/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_test_012_1/\n",
            "  inflating: train/meeting_en_test_012_1/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_012_1/transcript_MAN2_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_010/\n",
            "  inflating: train/meeting_cs_test_010/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_010/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_012_2/\n",
            "  inflating: train/meeting_cs_train_012_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_012_2/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_005_1/\n",
            "  inflating: train/meeting_cs_dev_005_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_005_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_011/\n",
            "  inflating: train/meeting_cs_train_011/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_011/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_013_1/\n",
            "  inflating: train/meeting_en_test_013_1/minutes_GENER_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_013_1/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_072/\n",
            "  inflating: train/meeting_en_train_072/transcript_MAN2_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_072/minutes_GENER_annot11.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_006_1/\n",
            "  inflating: train/meeting_en_dev_006_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_006_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_022_2/\n",
            "  inflating: train/meeting_cs_train_022_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_022_2/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_004_1/\n",
            "  inflating: train/meeting_cs_train_004_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_004_1/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_011_1/\n",
            "  inflating: train/meeting_en_train_011_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_011_1/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_en_train_025/\n",
            "  inflating: train/meeting_en_train_025/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_025/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_en_train_078_1/\n",
            "  inflating: train/meeting_en_train_078_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_078_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_014_1/\n",
            "  inflating: train/meeting_en_train_014_1/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_014_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_010/\n",
            "  inflating: train/meeting_en_test_010/transcript_MAN_annot03.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_010/minutes_GENER_annot03.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_004_2/\n",
            "  inflating: train/meeting_en_dev_004_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_004_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_001/\n",
            "  inflating: train/meeting_en_dev_001/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_001/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_032/\n",
            "  inflating: train/meeting_en_train_032/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_032/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_039_1/\n",
            "  inflating: train/meeting_en_train_039_1/transcript_MAN2_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_039_1/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_001/\n",
            "  inflating: train/meeting_cs_train_001/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_001/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_074/\n",
            "  inflating: train/meeting_en_train_074/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_074/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_023_1/\n",
            "  inflating: train/meeting_cs_train_023_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_023_1/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_064_1/\n",
            "  inflating: train/meeting_en_train_064_1/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_064_1/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_test_009_1/\n",
            "  inflating: train/meeting_en_test_009_1/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_009_1/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_006/\n",
            "  inflating: train/meeting_en_train_006/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_006/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_038/\n",
            "  inflating: train/meeting_en_train_038/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_038/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_4/\n",
            "  inflating: train/meeting_en_test_002_4/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_4/transcript_MAN2_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_005_1/\n",
            "  inflating: train/meeting_en_dev_005_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_005_1/transcript_MAN_annot16.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_012_1/\n",
            "  inflating: train/meeting_cs_test_012_1/minutes_GENER_annot19.txt.txt  \n",
            "  inflating: train/meeting_cs_test_012_1/transcript_MAN_annot19.txt.txt  \n",
            "   creating: train/meeting_cs_test_004_1/\n",
            "  inflating: train/meeting_cs_test_004_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_004_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_079/\n",
            "  inflating: train/meeting_en_train_079/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_079/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_079_2/\n",
            "  inflating: train/meeting_en_train_079_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_079_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_024_1/\n",
            "  inflating: train/meeting_en_train_024_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_024_1/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_train_029_1/\n",
            "  inflating: train/meeting_en_train_029_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_029_1/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_004_2/\n",
            "  inflating: train/meeting_cs_test_004_2/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_004_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_062_1/\n",
            "  inflating: train/meeting_en_train_062_1/minutes_GENER_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_062_1/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_031/\n",
            "  inflating: train/meeting_cs_train_031/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_031/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_003_1/\n",
            "  inflating: train/meeting_cs_dev_003_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_003_1/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_001/\n",
            "  inflating: train/meeting_cs_dev_001/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_001/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_015/\n",
            "  inflating: train/meeting_en_train_015/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_015/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_033_1/\n",
            "  inflating: train/meeting_cs_train_033_1/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_033_1/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/.DS_Store         \n",
            "   creating: train/meeting_cs_dev_003/\n",
            "  inflating: train/meeting_cs_dev_003/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_003/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_017/\n",
            "  inflating: train/meeting_en_test_017/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_017/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_train_017/\n",
            "  inflating: train/meeting_en_train_017/transcript_MAN_annot03.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_017/minutes_GENER_annot03.deidentified.txt  \n",
            "   creating: train/meeting_en_train_082/\n",
            "  inflating: train/meeting_en_train_082/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_082/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_007/\n",
            "  inflating: train/meeting_cs_test_007/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_007/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_015/\n",
            "  inflating: train/meeting_cs_test_015/minutes_GENER_annot19.txt  \n",
            "  inflating: train/meeting_cs_test_015/transcript_MAN_annot19_deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_015/notations.txt.txt  \n",
            "  inflating: train/meeting_cs_test_015/transcript_MAN_annot19.txt  \n",
            "   creating: train/meeting_cs_train_017_1/\n",
            "  inflating: train/meeting_cs_train_017_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_017_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_005_1/\n",
            "  inflating: train/meeting_cs_train_005_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_005_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_020_1/\n",
            "  inflating: train/meeting_cs_train_020_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_020_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_002/\n",
            "  inflating: train/meeting_en_dev_002/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_002/transcript_MAN_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_014_1/\n",
            "  inflating: train/meeting_cs_test_014_1/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_014_1/minutes_GENER_annot05.txt  \n",
            "   creating: train/meeting_en_dev_007_1/\n",
            "  inflating: train/meeting_en_dev_007_1/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_007_1/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_023/\n",
            "  inflating: train/meeting_cs_train_023/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_023/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_028/\n",
            "  inflating: train/meeting_en_train_028/minutes_GENER_annot02.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_028/transcript_MAN_annot15.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_015_1/\n",
            "  inflating: train/meeting_cs_test_015_1/minutes_GENER_annot19_deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_015_1/transcript_MAN_annot19.txt  \n",
            "   creating: train/meeting_en_test_002_3/\n",
            "  inflating: train/meeting_en_test_002_3/minutes_GENER_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_3/transcript_MAN2_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_test_007_2/\n",
            "  inflating: train/meeting_en_test_007_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_007_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_001/\n",
            "  inflating: train/meeting_en_train_001/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_001/transcript_MAN2_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_022/\n",
            "  inflating: train/meeting_en_train_022/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_022/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_022_1/\n",
            "  inflating: train/meeting_cs_train_022_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_022_1/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_078/\n",
            "  inflating: train/meeting_en_train_078/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_078/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_005/\n",
            "  inflating: train/meeting_cs_train_005/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_005/minutes_GENER_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_080_2/\n",
            "  inflating: train/meeting_en_train_080_2/transcript_MAN2_annot12.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_080_2/minutes_GENER_annot01.deidentified.txt  \n",
            "   creating: train/meeting_en_train_019/\n",
            "  inflating: train/meeting_en_train_019/minutes_GENER_annot14.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_019/transcript_MAN_annot14.deidentified.txt  \n",
            "   creating: train/meeting_en_train_020_1/\n",
            "  inflating: train/meeting_en_train_020_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_020_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_029_1/\n",
            "  inflating: train/meeting_cs_train_029_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_029_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_016_1/\n",
            "  inflating: train/meeting_en_test_016_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_016_1/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_001_1/\n",
            "  inflating: train/meeting_en_dev_001_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_001_1/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_022/\n",
            "  inflating: train/meeting_cs_train_022/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_022/transcript_MAN2_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_train_069_2/\n",
            "  inflating: train/meeting_en_train_069_2/transcript_MAN2_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_069_2/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_006/\n",
            "  inflating: train/meeting_cs_test_006/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_006/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_013/\n",
            "  inflating: train/meeting_cs_test_013/transcript_MAN_annot08.deidentified.txt  \n",
            " extracting: train/meeting_cs_test_013/readme.txt  \n",
            "  inflating: train/meeting_cs_test_013/minutes_GENER_annot08.deidentified.tx.txt  \n",
            "  inflating: train/meeting_cs_test_013/transcript_MAN_annot08.txt.txt  \n",
            "  inflating: train/meeting_cs_test_013/notations.txt  \n",
            "   creating: train/meeting_en_train_050/\n",
            "  inflating: train/meeting_en_train_050/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_050/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_013_2/\n",
            "  inflating: train/meeting_cs_train_013_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_013_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_047_1/\n",
            "  inflating: train/meeting_en_train_047_1/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_047_1/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_005_1/\n",
            "  inflating: train/meeting_cs_test_005_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_005_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_009/\n",
            "  inflating: train/meeting_cs_test_009/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_009/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_027/\n",
            "  inflating: train/meeting_cs_train_027/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_027/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_016/\n",
            "  inflating: train/meeting_cs_test_016/minutes_GENER_annot05.deidentified.txt  \n",
            " extracting: train/meeting_cs_test_016/readme.txt  \n",
            "  inflating: train/meeting_cs_test_016/transcript_MAN_annot05.txt  \n",
            "  inflating: train/meeting_cs_test_016/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_016/notations.txt  \n",
            "   creating: train/meeting_en_test_013_2/\n",
            "  inflating: train/meeting_en_test_013_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_013_2/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_3/\n",
            "  inflating: train/meeting_en_train_085_3/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_3/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_test_004/\n",
            "  inflating: train/meeting_en_test_004/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_004/transcript_MAN2_annot14.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_005/\n",
            "  inflating: train/meeting_cs_dev_005/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_005/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_068/\n",
            "  inflating: train/meeting_en_train_068/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_068/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_test_018_1/\n",
            "  inflating: train/meeting_en_test_018_1/transcript_MAN_annot23.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_018_1/minutes_GENER_annot23.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_2/\n",
            "  inflating: train/meeting_en_train_085_2/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_2/minutes_GENER_annot98.deidentified.txt  \n",
            "   creating: train/meeting_en_train_083_1/\n",
            "  inflating: train/meeting_en_train_083_1/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_083_1/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_069_1/\n",
            "  inflating: train/meeting_en_train_069_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_069_1/transcript_MAN2_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_006/\n",
            "  inflating: train/meeting_en_dev_006/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_006/minutes_GENER_annot07.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_5/\n",
            "  inflating: train/meeting_en_test_002_5/transcript_MAN2_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_5/minutes_GENER_annot16.deidentified.txt  \n",
            "   creating: train/meeting_en_test_001/\n",
            "  inflating: train/meeting_en_test_001/transcript_MAN2_annot24.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_001/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_021/\n",
            "  inflating: train/meeting_cs_train_021/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_021/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_en_test_008_2/\n",
            "  inflating: train/meeting_en_test_008_2/transcript_MAN2_annot22.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_008_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_038_2/\n",
            "  inflating: train/meeting_en_train_038_2/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_038_2/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_013_1/\n",
            "  inflating: train/meeting_en_train_013_1/transcript_MAN2_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_013_1/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_train_044/\n",
            "  inflating: train/meeting_en_train_044/minutes_GENER_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_044/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_008_1/\n",
            "  inflating: train/meeting_en_train_008_1/minutes_GENER_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_008_1/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_002_1/\n",
            "  inflating: train/meeting_cs_test_002_1/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_002_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_072_1/\n",
            "  inflating: train/meeting_en_train_072_1/transcript_MAN2_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_072_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_009/\n",
            "  inflating: train/meeting_en_train_009/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_009/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_005_2/\n",
            "  inflating: train/meeting_en_dev_005_2/minutes_GENER_annot16.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_005_2/transcript_MAN_annot16.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_4/\n",
            "  inflating: train/meeting_en_dev_010_4/minutes_GENER_annot99.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_4/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_en_train_030/\n",
            "  inflating: train/meeting_en_train_030/transcript_MAN_annot01.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_030/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_007/\n",
            "  inflating: train/meeting_cs_dev_007/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_007/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_054/\n",
            "  inflating: train/meeting_en_train_054/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_054/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_015_1/\n",
            "  inflating: train/meeting_cs_train_015_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_015_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_011_2/\n",
            "  inflating: train/meeting_cs_train_011_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_011_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_020/\n",
            "  inflating: train/meeting_en_train_020/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_020/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_025/\n",
            "  inflating: train/meeting_cs_train_025/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_025/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_029/\n",
            "  inflating: train/meeting_en_train_029/minutes_GENER_annot20.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_029/transcript_MAN_annot20.deidentified.txt  \n",
            "   creating: train/meeting_en_train_003/\n",
            "  inflating: train/meeting_en_train_003/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_003/transcript_MAN2_annot13.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_003/\n",
            "  inflating: train/meeting_en_dev_003/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_003/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_040_1/\n",
            "  inflating: train/meeting_en_train_040_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_040_1/transcript_MAN_annot10.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_013_1/\n",
            "  inflating: train/meeting_cs_train_013_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_013_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_012/\n",
            " extracting: train/meeting_cs_test_012/readme.txt  \n",
            "  inflating: train/meeting_cs_test_012/minutes_GENER_annot19.deidentified.txt.txt  \n",
            "  inflating: train/meeting_cs_test_012/notations.txt.txt  \n",
            "  inflating: train/meeting_cs_test_012/transcript_MAN_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_012/transcript_MAN_annot19.txt.txt  \n",
            "   creating: train/meeting_en_train_018_1/\n",
            "  inflating: train/meeting_en_train_018_1/transcript_MAN2_annot23.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_018_1/minutes_GENER_annot23.deidentified.txt  \n",
            "   creating: train/meeting_en_test_011/\n",
            "  inflating: train/meeting_en_test_011/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_011/transcript_MAN_annot24.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_009/\n",
            "  inflating: train/meeting_cs_dev_009/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_009/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_test_008_1/\n",
            "  inflating: train/meeting_en_test_008_1/transcript_MAN2_annot22.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_008_1/minutes_GENER_annot22.deidentified.txt  \n",
            "   creating: train/meeting_en_train_046_1/\n",
            "  inflating: train/meeting_en_train_046_1/minutes_GENER_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_046_1/transcript_MAN_annot11.deidentified.txt  \n",
            "   creating: train/meeting_en_test_004_1/\n",
            "  inflating: train/meeting_en_test_004_1/transcript_MAN2_annot14.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_004_1/minutes_GENER_annot14.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_009_2/\n",
            "  inflating: train/meeting_en_dev_009_2/transcript_MAN2_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_009_2/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_train_010/\n",
            "  inflating: train/meeting_en_train_010/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_010/transcript_MAN_annot12.deidentified.txt  \n",
            "   creating: train/meeting_en_train_079_1/\n",
            "  inflating: train/meeting_en_train_079_1/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_079_1/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_085_6/\n",
            "  inflating: train/meeting_en_train_085_6/transcript_MAN_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_085_6/minutes_GENER_annot02.deidentified.txt  \n",
            "   creating: train/meeting_en_test_002_1/\n",
            "  inflating: train/meeting_en_test_002_1/minutes_GENER_annot00.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_002_1/transcript_MAN2_annot20.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_014/\n",
            "  inflating: train/meeting_cs_test_014/minutes_GENER_annot05.deidentified.txt  \n",
            " extracting: train/meeting_cs_test_014/readme.txt  \n",
            "  inflating: train/meeting_cs_test_014/transcript_MAN_annot05.txt  \n",
            "  inflating: train/meeting_cs_test_014/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_014/notations.txt  \n",
            "   creating: train/meeting_cs_train_010_2/\n",
            "  inflating: train/meeting_cs_train_010_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_010_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_en_train_066/\n",
            "  inflating: train/meeting_en_train_066/transcript_MAN_annot08.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_066/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_003/\n",
            "  inflating: train/meeting_cs_train_003/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_003/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_077_2/\n",
            "  inflating: train/meeting_en_train_077_2/transcript_MAN_annot13.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_077_2/minutes_GENER_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_011/\n",
            "  inflating: train/meeting_cs_test_011/minutes_GENER_annot05.deidentified.txt  \n",
            " extracting: train/meeting_cs_test_011/readme.txt  \n",
            "  inflating: train/meeting_cs_test_011/transcript_MAN_annot05.txt  \n",
            "  inflating: train/meeting_cs_test_011/transcript_MAN_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_011/notations.txt  \n",
            "   creating: train/meeting_cs_test_002/\n",
            "  inflating: train/meeting_cs_test_002/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_002/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_039_2/\n",
            "  inflating: train/meeting_en_train_039_2/transcript_MAN2_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_039_2/minutes_GENER_annot08.deidentified.txt  \n",
            "   creating: train/meeting_en_train_033/\n",
            "  inflating: train/meeting_en_train_033/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_033/transcript_MAN_annot13.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_002_2/\n",
            "  inflating: train/meeting_cs_dev_002_2/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_002_2/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_003_1/\n",
            "  inflating: train/meeting_cs_train_003_1/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_003_1/minutes_GENER_annot18.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_002_1/\n",
            "  inflating: train/meeting_cs_train_002_1/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_002_1/transcript_MAN_annot19.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_020_2/\n",
            "  inflating: train/meeting_cs_train_020_2/transcript_MAN2_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_020_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_008_2/\n",
            "  inflating: train/meeting_cs_test_008_2/transcript_MAN_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_008_2/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_012/\n",
            "  inflating: train/meeting_en_train_012/minutes_GENER_annot21.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_012/transcript_MAN_annot21.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_006_1/\n",
            "  inflating: train/meeting_cs_test_006_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_006_1/transcript_MAN_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_038_1/\n",
            "  inflating: train/meeting_en_train_038_1/transcript_MAN_annot09.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_038_1/minutes_GENER_annot09.deidentified.txt  \n",
            "   creating: train/meeting_en_test_014/\n",
            "  inflating: train/meeting_en_test_014/minutes_ORIG.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_014/transcript_MAN_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_002_1/\n",
            "  inflating: train/meeting_cs_dev_002_1/minutes_GENER_annot18.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_002_1/transcript_MAN2_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_013_1/\n",
            "  inflating: train/meeting_cs_test_013_1/minutes_GENER_annot08.txt  \n",
            "  inflating: train/meeting_cs_test_013_1/transcript_MAN_annot08.txt.txt  \n",
            "   creating: train/meeting_cs_train_020/\n",
            "  inflating: train/meeting_cs_train_020/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_020/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_train_075/\n",
            "  inflating: train/meeting_en_train_075/transcript_MAN3_annot06.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_075/minutes_ORIG.deidentified.txt  \n",
            "   creating: train/meeting_en_train_059/\n",
            "  inflating: train/meeting_en_train_059/transcript_MAN_annot17.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_059/minutes_GENER_annot17.deidentified.txt  \n",
            "   creating: train/meeting_cs_train_019/\n",
            "  inflating: train/meeting_cs_train_019/minutes_GENER_annot05.deidentified.txt  \n",
            "  inflating: train/meeting_cs_train_019/transcript_MAN2_annot18.deidentified.txt  \n",
            "   creating: train/meeting_en_test_017_2/\n",
            "  inflating: train/meeting_en_test_017_2/transcript_MAN2_annot07.deidentified.txt  \n",
            "  inflating: train/meeting_en_test_017_2/minutes_GENER_annot06.deidentified.txt  \n",
            "   creating: train/meeting_en_dev_010_1/\n",
            "  inflating: train/meeting_en_dev_010_1/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_dev_010_1/transcript_MAN2_annot21.deidentified.txt  \n",
            "   creating: train/meeting_en_train_055/\n",
            "  inflating: train/meeting_en_train_055/minutes_GENER_annot11.deidentified.txt  \n",
            "  inflating: train/meeting_en_train_055/transcript_MAN2_annot06.deidentified.txt  \n",
            "   creating: train/meeting_cs_test_009_1/\n",
            "  inflating: train/meeting_cs_test_009_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_test_009_1/transcript_MAN_annot05.deidentified.txt  \n",
            "   creating: train/meeting_cs_dev_009_1/\n",
            "  inflating: train/meeting_cs_dev_009_1/minutes_GENER_annot19.deidentified.txt  \n",
            "  inflating: train/meeting_cs_dev_009_1/transcript_MAN_annot05.deidentified.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Kaggle/Cpastone_text_minuting/AutoMin_combined_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqjcktMTRFRI"
      },
      "outputs": [],
      "source": [
        "path='/content/train'\n",
        "filelist=os.listdir(path)\n",
        "\n",
        "automin_transcripts={}\n",
        "automin_summary={}\n",
        "participants={}\n",
        "for file in filelist:\n",
        "    summs={}\n",
        "    if file[8:10]=='en' or file[8:10]=='cs':\n",
        "        f=os.listdir(f'{path}/{file}')\n",
        "        for ts in f:\n",
        "            if ts[-4:]!='.txt':\n",
        "                continue\n",
        "            #print(ts[:7])\n",
        "            \n",
        "            if ts[:10]=='transcript':\n",
        "                name=list(ts.split('_'))[2][:8]\n",
        "                with open(f'{path}/{file}/{ts}', 'r', encoding='utf8') as f1:\n",
        "                    txt=f1.read()\n",
        "                    automin_transcripts[file[:-3]]=txt\n",
        "                    \n",
        "            if ts[:7]=='minutes':\n",
        "                # print(ts)\n",
        "                try:\n",
        "                    name=''.join(list(ts.split('.'))[-3])\n",
        "                except:\n",
        "                    name=''.join(list(ts.split('.'))[0])\n",
        "                with open(f'{path}/{file}/{ts}', 'r', encoding='utf8') as f2:\n",
        "                    txt=f2.read()\n",
        "                    summs[name]=txt\n",
        "        automin_summary[file[:-3]]=summs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq_S7gvwRH6o"
      },
      "outputs": [],
      "source": [
        "(count_small_transcripts_automin, count_big_transcripts_automin, sum_small_transcripts_automin, \n",
        "sum_big_transcripts_automin, position_small_transcripts_automin, position_big_transcripts_automin, \n",
        "total_sent_transcripts_automin, total_words_transcripts_automin, total_refined_words_transcripts_automin,\n",
        "total_refined_unique_words_transcripts_automin, unique_dict_transcripts_automin, number_transcripts_automin,\n",
        "max_len_transcripts_automin, position_max_len_transcripts_automin, small_cont_transcripts_automin) = ami_mean(automin_transcripts, 5 , 10)\n",
        "\n",
        "count_small_asum_automin=[]\n",
        "count_big_asum_automin=[]\n",
        "sum_small_asum_automin=[]\n",
        "sum_big_asum_automin=[]\n",
        "position_small_asum_automin=[]\n",
        "position_big_asum_automin=[]\n",
        "total_sent_asum_automin=[]\n",
        "total_words_asum_automin=[]\n",
        "total_refined_words_asum_automin=[]\n",
        "total_refined_unique_words_asum_automin=[]\n",
        "unique_dict_asum_automin=[]\n",
        "number_asum_automin=[]\n",
        "max_len_asum_automin=[]\n",
        "position_max_len_asum_automin=[]\n",
        "small_cont_asum_automin=[]\n",
        "\n",
        "ll=[count_small_asum_automin, count_big_asum_automin, sum_small_asum_automin, \n",
        "sum_big_asum_automin, position_small_asum_automin, position_big_asum_automin, \n",
        "total_sent_asum_automin, total_words_asum_automin, total_refined_words_asum_automin,\n",
        "total_refined_unique_words_asum_automin, unique_dict_asum_automin, number_asum_automin,\n",
        "max_len_asum_automin, position_max_len_asum_automin, small_cont_asum_automin]\n",
        "\n",
        "for key in automin_summary.keys():\n",
        "    temp=ami_mean(automin_summary[key], 5,10)\n",
        "    for index in range(len(temp)):\n",
        "        ll[index].append(temp[index])\n",
        "    \n",
        "count_small_asum_automin=sum(count_small_asum_automin)\n",
        "count_big_asum_automin=sum(count_big_asum_automin)\n",
        "sum_small_asum_automin=sum(sum_small_asum_automin)\n",
        "sum_big_asum_automin=sum(sum_big_asum_automin)\n",
        "position_small_asum_automin=sum(position_small_asum_automin,[])\n",
        "position_big_asum_automin=sum(position_big_asum_automin,[])\n",
        "total_sent_asum_automin=sum(total_sent_asum_automin)\n",
        "total_words_asum_automin=sum(total_words_asum_automin)\n",
        "total_refined_words_asum_automin=sum(total_refined_words_asum_automin)\n",
        "total_refined_unique_words_asum_automin=sum(total_refined_unique_words_asum_automin)\n",
        "number_asum_automin=sum(number_asum_automin)\n",
        "max_len_asum_automin=sum(max_len_asum_automin,[])\n",
        "position_max_len_asum_automin=sum(position_max_len_asum_automin,[])\n",
        "small_cont_asum_automin=sum(small_cont_asum_automin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EYk8dTvRLNA",
        "outputId": "7521f669-b6b9-4399-daab-80f5d0bebc02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.535714285714286\n",
            "356.67857142857144\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# AutoMin Turns and number of speakers (using change in speaker) \"\"\"\n",
        "\n",
        "turns_list=[]\n",
        "speaker_list=[]\n",
        "\n",
        "for key in automin_transcripts.keys():\n",
        "    s=[]\n",
        "    check=False\n",
        "    turns=0\n",
        "    speaker=[]\n",
        "    for index,i in enumerate(automin_transcripts[key]):\n",
        "        if check == True:\n",
        "            s.append(i)\n",
        "        if i=='(' and automin_transcripts[key][index+1:index+7]=='PERSON':\n",
        "            turns+=1\n",
        "            check=True\n",
        "        if i==')' and check == True:\n",
        "            check=False\n",
        "            speaker.append(''.join(s[:-1]))\n",
        "            s=[]\n",
        "    turns_list.append(turns)\n",
        "    speaker_list.append(list(set(speaker)))\n",
        "\n",
        "# number of speakers\n",
        "\n",
        "print(len(sum(speaker_list, []))/ len(speaker_list))\n",
        "\n",
        "# turns\n",
        "\n",
        "print(sum(turns_list)/len(turns_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curhw-z7RNZe",
        "outputId": "58ecb8c0-1053-43d9-d8e9-f9dc4718bd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Novel summary words in AMI:  (304, 0.16712479384277076)\n",
            "Novel summary words in ICSI:  (33, 0.018141836173721827)\n",
            "Novel summary words in AutoMin:  (646, 0.35514018691588783)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# DataFrame\"\"\"\n",
        "\n",
        "df=pd.DataFrame(columns=['count_small', 'count_big','sum_small','sum_big','position_small','position_big',\n",
        "    'total_sent','total_words','total_refined_words','total_unique_refined_words','number','unique','max_len',\n",
        "                         'position_max_len','small_cont_percentage','dataset', 'label'],\n",
        "                index=['ami_transcripts', 'ami_esum', 'ami_asum','icsi_transcripts', 'icsi_asum','automin_transcripts', 'automin_asum'])\n",
        "df.count_small=[count_small_transcripts_ami, count_small_esum_ami, count_small_asum_ami,count_small_transcripts_icsi,count_small_asum_icsi, count_small_transcripts_automin, count_small_asum_automin]\n",
        "df.count_big=[count_big_transcripts_ami, count_big_esum_ami, count_big_asum_ami,count_big_transcripts_icsi, count_big_asum_icsi, count_big_transcripts_automin, count_big_asum_automin]\n",
        "df.sum_small=[sum_small_transcripts_ami, sum_small_esum_ami, sum_small_asum_ami, sum_small_transcripts_icsi,sum_small_asum_icsi, sum_small_transcripts_automin, sum_small_asum_automin]\n",
        "df.sum_big=[sum_big_transcripts_ami,sum_big_esum_ami, sum_big_asum_ami, sum_big_transcripts_icsi, sum_big_asum_icsi, sum_big_transcripts_automin, sum_big_asum_automin]\n",
        "df.position_small=[position_small_transcripts_ami,position_small_esum_ami, position_small_asum_ami, position_small_transcripts_icsi, position_small_asum_icsi, position_small_transcripts_automin, position_small_asum_automin]\n",
        "df.position_big=[position_big_transcripts_ami,position_big_esum_ami, position_big_asum_ami, position_big_transcripts_icsi, position_big_asum_icsi, position_big_transcripts_automin, position_big_asum_automin]\n",
        "df.total_sent=[total_sent_transcripts_ami, total_sent_esum_ami, total_sent_asum_ami, total_sent_transcripts_icsi, total_sent_asum_icsi, total_sent_transcripts_automin, total_sent_asum_automin]\n",
        "df.total_words=[total_words_transcripts_ami,total_words_esum_ami,total_words_asum_ami, total_words_transcripts_icsi, total_words_asum_icsi, total_words_transcripts_automin, total_words_asum_automin]\n",
        "df.total_refined_words=[total_refined_words_transcripts_ami, total_refined_words_esum_ami, total_refined_words_asum_ami, total_refined_words_transcripts_icsi, total_refined_words_asum_icsi, total_refined_words_transcripts_automin, total_refined_words_asum_automin]\n",
        "df.total_unique_refined_words=[total_refined_unique_words_transcripts_ami, total_refined_unique_words_esum_ami, total_refined_unique_words_asum_ami, total_refined_unique_words_transcripts_icsi, total_refined_unique_words_asum_icsi, total_refined_unique_words_transcripts_ami, total_refined_unique_words_asum_automin]\n",
        "df.number=[number_transcripts_ami,number_esum_ami, number_asum_ami, number_transcripts_icsi, number_asum_icsi, number_transcripts_automin, number_asum_automin]\n",
        "df.unique=[unique_dict_transcripts_ami, unique_dict_esum_ami, unique_dict_asum_ami, unique_dict_transcripts_icsi, unique_dict_asum_icsi, unique_dict_transcripts_automin, unique_dict_asum_automin]\n",
        "df.max_len=[max_len_transcripts_ami, max_len_esum_ami, max_len_asum_ami,max_len_transcripts_icsi,max_len_asum_icsi, max_len_transcripts_automin, max_len_asum_automin]\n",
        "df.position_max_len=[position_max_len_transcripts_ami, position_max_len_esum_ami, position_max_len_asum_ami,position_max_len_transcripts_icsi,position_max_len_asum_icsi, position_max_len_transcripts_automin, position_max_len_asum_automin]\n",
        "df.small_cont_percentage=[small_cont_transcripts_ami/total_sent_transcripts_ami, small_cont_esum_ami/total_sent_esum_ami, small_cont_asum_ami/total_sent_asum_ami,small_cont_transcripts_icsi/total_sent_transcripts_icsi, small_count_asum_icsi/total_sent_asum_icsi, small_cont_transcripts_automin/total_sent_transcripts_automin, small_cont_asum_automin/total_sent_asum_automin]\n",
        "\n",
        "df.dataset=['ami', 'ami','ami', 'icsi', 'icsi','automin', 'automin']\n",
        "df.label=['transcript', 'extractive_summary', 'abstractive_summary','transcript', 'abstractive_summary','transcript', 'abstractive_summary']\n",
        "\n",
        "# convert list of automin unique words to dict\n",
        "\n",
        "t={}\n",
        "for dic in df['unique']['automin_asum']:\n",
        "    for key in dic:\n",
        "        if key in t:\n",
        "            t[key]+=dic[key]\n",
        "        else:\n",
        "            t[key]=dic[key]\n",
        "df['unique']['automin_asum']=t\n",
        "\n",
        "\"\"\"# Novel Summary Words\"\"\"\n",
        "\n",
        "def novel_summ_word(df, dataset):\n",
        "    t=0\n",
        "    for i in df['unique'][f'{dataset}_asum'].keys():\n",
        "        if i not in df['unique'][f'{dataset}_transcripts'].keys():\n",
        "            t+=1\n",
        "            \n",
        "    return t, t/len(df.unique.ami_asum)\n",
        "\n",
        "print('Novel summary words in AMI: ', novel_summ_word(df, 'ami'))\n",
        "print('Novel summary words in ICSI: ',novel_summ_word(df, 'icsi'))\n",
        "print('Novel summary words in AutoMin: ',novel_summ_word(df,'automin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGru9D11RQy-",
        "outputId": "7f6db1ef-cb3c-4c8e-aa6c-a562851b9a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In terms of sentences\n",
            "ami_transcripts         553.080292\n",
            "ami_esum                 94.175182\n",
            "ami_asum                  8.503650\n",
            "icsi_transcripts       1667.967213\n",
            "icsi_asum                34.000000\n",
            "automin_transcripts     630.607143\n",
            "automin_asum             20.142857\n",
            "dtype: float64\n",
            "In terms of words\n",
            "ami_transcripts         5934.401460\n",
            "ami_esum                1981.598540\n",
            "ami_asum                 179.423358\n",
            "icsi_transcripts        9795.131148\n",
            "icsi_asum                658.000000\n",
            "automin_transcripts    10547.928571\n",
            "automin_asum             405.821429\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# Average length\"\"\"\n",
        "\n",
        "# In terms of sentences\n",
        "print('In terms of sentences')\n",
        "print(df.total_sent/df.number)\n",
        "# In terms of words\n",
        "print('In terms of words')\n",
        "print(df.total_words/df.number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCnDoFu_RUTK",
        "outputId": "ef6387e3-2714-4a4c-b079-354e452d5a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of sentences\n",
            "ami_transcripts         89.569343\n",
            "ami_esum                92.890511\n",
            "ami_asum                36.963504\n",
            "icsi_transcripts        44.724864\n",
            "icsi_asum               30.294118\n",
            "automin_transcripts     96.857143\n",
            "automin_asum           117.500000\n",
            "Name: max_len, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# Max sentence length\"\"\"\n",
        "\n",
        "print('Maximum length of sentences')\n",
        "print(df.max_len.apply(sum)/df.max_len.apply(len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIO-qwAERXp5",
        "outputId": "1d23d445-d195-4048-f65e-b5447dc89365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position of big sentences in ICSI vs AutoMin: \n",
            "Ttest_indResult(statistic=1.437982766874127, pvalue=0.1504495074470736)\n",
            "Position of big sentences in AMI vs AutoMin: \n",
            "Ttest_indResult(statistic=0.029841131849375886, pvalue=0.9761939230309229)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# T-test\"\"\"\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "print('Position of big sentences in ICSI vs AutoMin: ')\n",
        "print(ttest_ind(df.position_big.icsi_transcripts,df.position_big.automin_transcripts))\n",
        "print('Position of big sentences in AMI vs AutoMin: ')\n",
        "print(ttest_ind(df.position_big.ami_transcripts,df.position_big.automin_transcripts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmg-H456RaI7",
        "outputId": "099240e8-1e7f-4a6a-efdd-cd337fdcceaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AMI : 0.8496506489573399\n",
            "ICSI:  0.9449936459177705\n",
            "AutoMin: 0.6663630356572788\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# Correlation\"\"\"\n",
        "\n",
        "# Correlation between transcript and summary vocabulary\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "def correlation(dataset):\n",
        "    #normalize score\n",
        "    t_vocab={}\n",
        "    for key in df['unique'][f'{dataset}_transcripts'].keys():\n",
        "        t_vocab[key]=df['unique'][f'{dataset}_transcripts'][key]/len(df['unique'][f'{dataset}_transcripts'])\n",
        "\n",
        "    s_vocab={}\n",
        "    for key in df['unique'][f'{dataset}_asum'].keys():\n",
        "        s_vocab[key]=df['unique'][f'{dataset}_asum'][key]/len(df['unique'][f'{dataset}_asum'])\n",
        "\n",
        "    corr=[]\n",
        "    x=[]\n",
        "    y=[]\n",
        "    for key in t_vocab:\n",
        "        if key in s_vocab:\n",
        "            x.append(t_vocab[key])\n",
        "            y.append(s_vocab[key])\n",
        "\n",
        "    return pearsonr(x,y)[0]\n",
        "\n",
        "print('AMI :', correlation('ami'))\n",
        "print('ICSI: ', correlation('icsi'))\n",
        "print('AutoMin:', correlation('automin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxPqpZlRcm-",
        "outputId": "9d029810-668d-4d67-c1ab-8ec16353a90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"# LDA \"\"\"\n",
        "\n",
        "# Takes too long to run on colab\n",
        "\n",
        "# functions \n",
        "\n",
        "import spacy\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "\n",
        "spacy.load('en')\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "    \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jXywU-KtRfs-",
        "outputId": "5e8c5c29-4020-41c6-d4a3-29e66ab738a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average topics AMI:  14.751824817518248\n"
          ]
        }
      ],
      "source": [
        "#ami \n",
        "scores=0\n",
        "for key in transcripts.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        if len(tokens)>=5:\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=50, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=20)\n",
        "    a=[]\n",
        "    for topic in topics:\n",
        "        #print(topic[1].split('+ '))\n",
        "        s=0\n",
        "        for i in topic[1].split('+ '):\n",
        "            s+=float(i[:5])\n",
        "        a.append(s)\n",
        "    maxa=max(a)\n",
        "    a=[i/maxa for i in a if i/maxa>0.5]\n",
        "    scores+=len(a)\n",
        "    \n",
        "print('Average topics AMI: ',scores/len(transcripts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYNQ82NURjA8"
      },
      "outputs": [],
      "source": [
        "#icsi\n",
        "\n",
        "scores=0\n",
        "for index in range(len(text_list)):\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(''.join(text_list[index])):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=20)\n",
        "    a=[]\n",
        "    for topic in topics:\n",
        "        #print(topic[1].split('+ '))\n",
        "        s=0\n",
        "        for i in topic[1].split('+ '):\n",
        "            s+=float(i[:5])\n",
        "        a.append(s)\n",
        "    maxa=max(a)\n",
        "    a=[i/maxa for i in a if i/maxa>0.5]\n",
        "    scores+=len(a)\n",
        "\n",
        "    \n",
        "print('Average topics ICSI: ',scores/len(text_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBPAi0mARq55"
      },
      "outputs": [],
      "source": [
        "# automin\n",
        "\n",
        "scores=0\n",
        "for key in automin_transcripts.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=20)\n",
        "    a=[]\n",
        "    for topic in topics:\n",
        "        #print(topic[1].split('+ '))\n",
        "        s=0\n",
        "        for i in topic[1].split('+ '):\n",
        "            s+=float(i[:5])\n",
        "        a.append(s)\n",
        "    maxa=max(a)\n",
        "    a=[i/maxa for i in a if i/maxa>0.5]\n",
        "    scores+=len(a)\n",
        "    \n",
        "print('Average topics AutoMin: ',scores/len(text_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-t_rlfvRs5L"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Inter Annotator Agreement \"\"\"\n",
        "\n",
        "# Due to use of LDA, running again may give slightly different results\n",
        "\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "chisq=[]\n",
        "for key1 in automin_summary.keys():\n",
        "    scores=0\n",
        "    og=[]\n",
        "    full_dioc={}\n",
        "    dioc=[]\n",
        "    for key in automin_summary[key1].keys():\n",
        "        text_data = []\n",
        "        a=[]\n",
        "        for line in nltk.sent_tokenize(automin_summary[key1][key]):\n",
        "            tokens = prepare_text_for_lda(line)\n",
        "            # print(tokens)\n",
        "            if len(tokens)>=5:\n",
        "                #print(tokens)\n",
        "                text_data.append(tokens)\n",
        "        dictionary = corpora.Dictionary(text_data)\n",
        "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "        #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "        #dictionary.save('dictionary.gensim')\n",
        "        ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "        topics = ldamodel.print_topics(num_words=10)\n",
        "        topic_list=[]\n",
        "        topc_dic={}\n",
        "        for topic in topics:\n",
        "            #print(topic[1].split('+'))\n",
        "            for i in topic[1].split('+'):\n",
        "                a.append(i[7:-1])\n",
        "                name=i[7:-1]\n",
        "                if name in dioc:\n",
        "                    full_dioc[name]+=1\n",
        "                else:\n",
        "                    full_dioc[name]=1\n",
        "                if name in topc_dic:\n",
        "                    topc_dic[name]+=1\n",
        "                else:\n",
        "                    topc_dic[name]=1\n",
        "\n",
        "            #a.append(topic[1].split('+')[7:-1])\n",
        "            topic_list.append(a)\n",
        "        dioc.append(topc_dic)\n",
        "        og.append(topic_list)\n",
        "    vals=[]\n",
        "    for i in range(len(dioc)):\n",
        "        #print(i)\n",
        "        x_obs=list(dioc[i].values())\n",
        "        y_exp=[]\n",
        "        for key in dioc[i]:\n",
        "            if key in full_dioc:\n",
        "                y_exp.append(full_dioc[key])\n",
        "            else:\n",
        "                y_exp.append(0)\n",
        "        vals.append(chisquare(x_obs,y_exp)[1])\n",
        "    chisq.append(statistics.median(vals))\n",
        "print('Median: ', statistics.median(chisq))\n",
        "print('Mean: ', statistics.mean(chisq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wzPoGm1RzX_"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/gcunhase/AMICorpusXML.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxQnWVNbSlWo"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Perplexity and Entropy \"\"\"\n",
        "\n",
        "# find number of words spoken by each speaker\n",
        "# ami\n",
        "path='/content/AMICorpusXML/data/ami-transcripts-speaker'\n",
        "filelist=os.listdir(path)\n",
        "entropy=[]\n",
        "perplexity=[]\n",
        "for key in transcripts.keys():\n",
        "    speaker_words=[]\n",
        "    for file in filelist:\n",
        "        if file[:7]==key:\n",
        "            with open(f'{path}/{file}','r', encoding='utf8') as f:\n",
        "                txt=f.read()\n",
        "                speaker_words.append(len(txt.split(' ')))\n",
        "        elif file[:6]==key:\n",
        "            with open(f'{path}/{file}','r', encoding='utf8') as f:\n",
        "                txt=f.read()\n",
        "                speaker_words.append(len(txt.split(' ')))\n",
        "        else:\n",
        "            pass\n",
        "    tot=sum(speaker_words)\n",
        "    speaker_words=[i/tot for i in speaker_words]\n",
        "    e=0\n",
        "    for p in speaker_words:\n",
        "        e+=(np.log2(p))\n",
        "    e=e/len(speaker_words)\n",
        "    entropy.append(e)\n",
        "    perplexity.append(pow(2,e))\n",
        "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
        "print('AVG Entropy: ',statistics.mean(entropy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZDH-MxTSsmF"
      },
      "outputs": [],
      "source": [
        "# icsi\n",
        "\n",
        "speakers={}\n",
        "entropy=[]\n",
        "perplexity=[]\n",
        "for root in root_list:\n",
        "    \n",
        "    #getting number of words spoken by speakers\n",
        "    \n",
        "    for child in root[1]:\n",
        "        if 'Participant' in child.attrib:\n",
        "                #print(len(child.text.split(' ')))\n",
        "                if child.attrib['Participant'] in speakers:\n",
        "                    speakers[child.attrib['Participant']]+=len(child.text.split(' '))\n",
        "                else:\n",
        "                    speakers[child.attrib['Participant']]=len(child.text.split(' '))\n",
        "        #print(child)\n",
        "    \n",
        "    tot=sum(speakers.values())\n",
        "    speaker=[i/tot for i in speakers.values()]\n",
        "    e=0\n",
        "    for p in speaker:\n",
        "        e+=(np.log2(p))\n",
        "    e=e/len(speaker)\n",
        "    entropy.append(e)\n",
        "    perplexity.append(pow(2,e))\n",
        "\n",
        "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
        "print('AVG Entropy: ',statistics.mean(entropy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Epdm1RSu1P"
      },
      "outputs": [],
      "source": [
        "# automin\n",
        "entropy=[]\n",
        "perplexity=[]\n",
        "\n",
        "for key in automin_transcripts.keys():\n",
        "    turns=0\n",
        "    speaker={}\n",
        "    text=automin_transcripts[key]#.split(' ')\n",
        "    # get speaker words\n",
        "    for index,i in enumerate(text):\n",
        "        \n",
        "        \n",
        "        if i=='(' and text[index+1:index+7]=='PERSON':\n",
        "            name=text[index+1:index+8]\n",
        "            t=[]\n",
        "            for index_temp,temp in enumerate(text[(index+10):]):\n",
        "                if temp=='(':\n",
        "                    break\n",
        "                t.append(temp)\n",
        "            t=len((''.join(t)).split(' '))\n",
        "            if name in speaker:\n",
        "                speaker[name]+=t\n",
        "            else:\n",
        "                speaker[name]=t\n",
        "    try:\n",
        "        tot=sum(speaker.values())\n",
        "        speaker=[i/tot for i in speaker.values()]\n",
        "        e=0\n",
        "        for p in speaker:\n",
        "            e+=(np.log2(p))\n",
        "        e=e/len(speaker)\n",
        "        entropy.append(e)\n",
        "        perplexity.append(pow(2,e))\n",
        "    except:\n",
        "        pass\n",
        "        #print(key)\n",
        "                    \n",
        "\n",
        "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
        "print('AVG Entropy: ',statistics.mean(entropy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xefnPU_LSzyv"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Standard Deviation between length of vocabularies of two summaries  \"\"\"\n",
        "\n",
        "stopWords = list(set(stopwords.words(\"english\")))\n",
        "standard_devs=[]\n",
        "for key1 in automin_summary.keys():\n",
        "    lens=[]\n",
        "    for key in automin_summary[key1].keys():\n",
        "        text_data = []\n",
        "        a=[]\n",
        "        vocab={}\n",
        "        for line in nltk.sent_tokenize(automin_summary[key1][key]):\n",
        "            words = nltk.word_tokenize(line)\n",
        "            words=[word.lower() for word in words]\n",
        "            refined_words=[word for word in words if word not in stopWords]\n",
        "            for word in refined_words:\n",
        "                if word in vocab:\n",
        "                    vocab[word]+=1\n",
        "                else:\n",
        "                    vocab[word]=1\n",
        "            \n",
        "        lens.append(len(vocab.keys()))\n",
        "    if len(lens)==1:\n",
        "        standard_devs.append(0)\n",
        "    else:\n",
        "        standard_devs.append(statistics.stdev(lens))\n",
        "print('Average of std between size of vocab: ', statistics.mean(standard_devs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU6X-VbiS5Wi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"# Similarity between topics in Transcript and summaries \"\"\"\n",
        "\n",
        "def get_jaccard_sim(l1, l2): \n",
        "    a = set(l1) \n",
        "    b = set(l2)\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6RScoW1TfyM"
      },
      "outputs": [],
      "source": [
        "#ami \n",
        "similarity=[]\n",
        "for key in extractive_summary.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    \n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    for line in nltk.sent_tokenize(extractive_summary[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    \n",
        "    s_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            s_topic.append(topic[1].split('\"')[index])\n",
        "    similarity.append(get_jaccard_sim(t_topic, s_topic))\n",
        "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT1mC5EtTh_0"
      },
      "outputs": [],
      "source": [
        "#icsi\n",
        "similarity=[]\n",
        "for index in range(len(text_list)):\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(''.join(text_list[index])):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=20)\n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    for line in nltk.sent_tokenize(''.join(ab_sumtext_list[index])):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    s_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            s_topic.append(topic[1].split('\"')[index])\n",
        "    similarity.append(get_jaccard_sim(t_topic, s_topic))\n",
        "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8LdrNyLTlaZ",
        "outputId": "0e83a59f-4a28-4b39-bfbb-14cbeeb80167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jaccard similarity between transcript and summary topics:  0.10212787516748172\n"
          ]
        }
      ],
      "source": [
        "#automin\n",
        "\n",
        "similarity=[]\n",
        "for key in automin_transcripts.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    text_data=[]\n",
        "    for key1 in automin_summary[key]:\n",
        "        for line in nltk.sent_tokenize(automin_summary[key][key1]):\n",
        "            tokens = prepare_text_for_lda(line)\n",
        "            if len(tokens)>=5:\n",
        "                text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    s_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            s_topic.append(topic[1].split('\"')[index])\n",
        "    similarity.append(get_jaccard_sim(t_topic, s_topic))\n",
        "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "B_9i4wMNUXIi",
        "outputId": "50b3cddf-c5d6-45c9-8f2b-24fa087cfa3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f92c08d75d0>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"# Position bias\"\"\"\n",
        "\n",
        "# ami\n",
        "\n",
        "final_scores=[0]*100\n",
        "for key in extractive_summary.keys():\n",
        "    unique={}\n",
        "    # creating vocab\n",
        "    txt=nltk.sent_tokenize(extractive_summary[key])\n",
        "    for index,sentence in enumerate(txt):\n",
        "        temp=nltk.word_tokenize(sentence)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        for word in word_tokens_refined:\n",
        "            if word not in unique: \n",
        "                unique[word]=1\n",
        "            elif word in unique:\n",
        "                unique[word]+=1\n",
        "    sentence_scores=[]\n",
        "    txt=nltk.sent_tokenize(transcripts[key])\n",
        "    \n",
        "    # scoring each sentence in transcript\n",
        "    for index,sentence in enumerate(txt):\n",
        "        score=0\n",
        "        temp=nltk.word_tokenize(sentence)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        for word in word_tokens_refined:\n",
        "            if word in unique: \n",
        "                score+=1\n",
        "        sentence_scores.append(score)\n",
        "    # creating 100 bins\n",
        "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
        "        try:\n",
        "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
        "        except:\n",
        "            final_scores[index]+=sum(sentence_scores[i:])\n",
        "\n",
        "\n",
        "sns.lineplot(x=list(range(100)),y=final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "h7MuuONpUiw6",
        "outputId": "27c8b1cb-639e-4d22-cc29-16f6d70bddb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f92c4979250>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# icsi\n",
        "\n",
        "final_scores=[0]*100\n",
        "for key in range(len(text_list)):\n",
        "    unique={}\n",
        "    # creating vocab\n",
        "    txt=nltk.sent_tokenize(''.join(ab_sumtext_list[key]))\n",
        "    for index,sentence in enumerate(txt):\n",
        "        temp=nltk.word_tokenize(sentence)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        for word in word_tokens_refined:\n",
        "            if word not in unique: \n",
        "                unique[word]=1\n",
        "            elif word in unique:\n",
        "                unique[word]+=1\n",
        "    \n",
        "    sentence_scores=[]\n",
        "    txt=nltk.sent_tokenize(''.join(text_list[key]))\n",
        "    \n",
        "    # scoring each sentence in transcript\n",
        "    for index,sentence in enumerate(txt):\n",
        "        score=0\n",
        "        temp=nltk.word_tokenize(sentence)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        for word in word_tokens_refined:\n",
        "            if word in unique: \n",
        "                score+=1\n",
        "        sentence_scores.append(score)\n",
        "    # creating 100 bins\n",
        "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
        "        try:\n",
        "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
        "        except:\n",
        "            final_scores[index]+=sum(sentence_scores[i:])\n",
        "\n",
        "sns.lineplot(x=list(range(100)),y=final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "iK_xIshyUlRo",
        "outputId": "5d3e221a-00a0-44c7-f379-f03a8412acf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f92c0b9bb10>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# automin\n",
        "\n",
        "\n",
        "final_scores=[0]*100\n",
        "for key in automin_summary.keys():\n",
        "    unique={}\n",
        "    # creating vocab\n",
        "    for key1 in automin_summary[key].keys():\n",
        "        txt=nltk.sent_tokenize(automin_summary[key][key1])\n",
        "        for index,sentence in enumerate(txt):\n",
        "            temp=nltk.word_tokenize(sentence)\n",
        "            temp=[word.lower() for word in temp]\n",
        "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "            for word in word_tokens_refined:\n",
        "                if word not in unique: \n",
        "                    unique[word]=1\n",
        "                elif word in unique:\n",
        "                    unique[word]+=1\n",
        "    \n",
        "    sentence_scores=[]\n",
        "    try:\n",
        "        txt=nltk.sent_tokenize(automin_transcripts[key])\n",
        "        # scoring each sentence in transcript\n",
        "        for index,sentence in enumerate(txt):\n",
        "            score=0\n",
        "            temp=nltk.word_tokenize(sentence)\n",
        "            temp=[word.lower() for word in temp]\n",
        "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "            for word in word_tokens_refined:\n",
        "                if word in unique: \n",
        "                    score+=1\n",
        "            sentence_scores.append(score)\n",
        "    except:\n",
        "        pass\n",
        "    # creating 100 bins\n",
        "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
        "        try:\n",
        "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
        "        except:\n",
        "            final_scores[index]+=sum(sentence_scores[i:])\n",
        "\n",
        "\n",
        "sns.lineplot(x=list(range(100)),y=final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz1cchDsUoUR",
        "outputId": "6c86dc98-662f-4b85-ef9f-63b071050bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.635036496350365"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"# Occurance of important sentences \"\"\"\n",
        "\n",
        "#ami \n",
        "\n",
        "important=[]\n",
        "for key in extractive_summary.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    \n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    t_topic=[word for word in t_topic if word not in stopWords]\n",
        "    scores=[]\n",
        "    for index,line in enumerate(nltk.sent_tokenize(transcripts[key])):\n",
        "        temp=nltk.word_tokenize(line)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        score=0\n",
        "        for word in word_tokens_refined:\n",
        "            if word in t_topic:\n",
        "                score+=1\n",
        "        scores.append(score)\n",
        "    max_score=max(scores)\n",
        "    scores=[i/max_score for i in scores if i/max_score>0.7]\n",
        "    \n",
        "    important.append(len(scores))\n",
        "statistics.mean(important)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOSn4k0_VRmr",
        "outputId": "e5319fb5-a4cd-4847-f696-8615ea138414"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.1475409836065573"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# icsi \n",
        "\n",
        "important=[]\n",
        "for key in range(len(text_list)):\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(''.join(text_list[key])):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    \n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    t_topic=[word for word in t_topic if word not in stopWords]\n",
        "    scores=[]\n",
        "    for index,line in enumerate(nltk.sent_tokenize(''.join(text_list[key]))):\n",
        "        temp=nltk.word_tokenize(line)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        score=0\n",
        "        for word in word_tokens_refined:\n",
        "            if word in t_topic:\n",
        "                score+=1\n",
        "        scores.append(score)\n",
        "    max_score=max(scores)\n",
        "    scores=[i/max_score for i in scores if i/max_score>0.7]\n",
        "    \n",
        "    important.append(len(scores))\n",
        "statistics.mean(important)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI9OxFeaVV08",
        "outputId": "7817c028-7a34-4b4d-e487-016f7d56b978"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.035714285714286"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#automin\n",
        "\n",
        "important=[]\n",
        "for key in automin_transcripts.keys():\n",
        "    text_data = []\n",
        "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
        "        tokens = prepare_text_for_lda(line)\n",
        "        # print(tokens)\n",
        "        if len(tokens)>=5:\n",
        "            #print(tokens)\n",
        "            text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    #dictionary.save('dictionary.gensim')\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
        "    topics = ldamodel.print_topics(num_words=10)\n",
        "    \n",
        "    \n",
        "    t_topic=[]\n",
        "    for topic in topics:\n",
        "        for index in range(1,len(topic[1].split('\"')),2):\n",
        "            t_topic.append(topic[1].split('\"')[index])\n",
        "    \n",
        "    t_topic=[word for word in t_topic if word not in stopWords]\n",
        "    scores=[]\n",
        "    for index,line in enumerate(nltk.sent_tokenize(automin_transcripts[key])):\n",
        "        temp=nltk.word_tokenize(line)\n",
        "        temp=[word.lower() for word in temp]\n",
        "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
        "        score=0\n",
        "        for word in word_tokens_refined:\n",
        "            if word in t_topic:\n",
        "                score+=1\n",
        "        scores.append(score)\n",
        "    max_score=max(scores)\n",
        "    scores=[i/max_score for i in scores if i/max_score>0.7]\n",
        "    \n",
        "    important.append(len(scores))\n",
        "statistics.mean(important)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARTVt2b-VaJ0"
      },
      "source": [
        "Error in Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQycsLj7VYw2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Minuting Datasets Raw Data Info.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}